<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title> Spidey | Spidey API Reference </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content=" Spidey | Spidey API Reference ">
      
      
      <link rel="icon" href="favicon.ico">
      <link rel="stylesheet" href="public/docfx.min.css">
      <link rel="stylesheet" href="public/main.css">
      <meta name="docfx:navrel" content="toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="">
      
      
      <meta name="docfx:docurl" content="https://github.com/JaCraig/Spidey/blob/master/docfx_project/index.md/#L1">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="index.html">
            <img id="logo" class="svg" src="images/icon.png" alt="Spidey">
            Spidey
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">

      <div class="content">
        <div class="actionbar">

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="">
<h1 id="-spidey"><img src="https://jacraig.github.io/Spidey/images/icon.png" style="height:25px" alt="Spidey Icon"> Spidey</h1>

<p><a href="https://github.com/JaCraig/Spidey/actions/workflows/dotnet-publish.yml"><img src="https://github.com/JaCraig/Spidey/actions/workflows/dotnet-publish.yml/badge.svg" alt=".NET Publish"></a> <a href="https://www.nuget.org/packages/Spidey/"><img src="https://img.shields.io/nuget/v/Spidey.svg" alt="NuGet"></a></p>
<p>Spidey is a flexible and extensible .NET library for crawling web content. It is designed for .NET Core applications and provides a modular architecture, allowing you to customize or extend any part of the crawling pipeline.</p>
<h2 id="features">Features</h2>
<ul>
<li>Simple API for crawling websites</li>
<li>Highly configurable via the <code>Options</code> class</li>
<li>Dependency injection support (IoC/DI)</li>
<li>Easily replaceable subsystems (engine, parser, scheduler, etc.)</li>
<li>Callback-based result handling</li>
<li>NuGet package available</li>
</ul>
<h2 id="quick-start">Quick Start</h2>
<p>Install the NuGet package:</p>
<pre><code class="lang-powershell">dotnet add package Spidey
</code></pre>
<h2 id="setting-up-the-library">Setting up the Library</h2>
<p>Register Spidey in your app's service collection using the <code>RegisterSpidey</code> extension method:</p>
<pre><code class="lang-csharp">using Microsoft.Extensions.DependencyInjection;
using Spidey;

var services = new ServiceCollection();
services.RegisterSpidey();

// Optionally, register your Options configuration
services.AddSingleton(new Options
{
    ItemFound = result =&gt; Console.WriteLine($&quot;Found: {result.Url}&quot;),
    Allow = new List&lt;string&gt; { &quot;http://mywebsite&quot;, &quot;http://mywebsite2&quot; },
    FollowOnly = new List&lt;string&gt; { /* regex patterns */ },
    Ignore = new List&lt;string&gt; { /* regex patterns */ },
    StartLocations = new List&lt;string&gt; { &quot;http://mywebsite&quot;, &quot;http://mywebsite2&quot; },
    UrlReplacements = new Dictionary&lt;string, string&gt; { /* { &quot;old&quot;, &quot;new&quot; } */ },
    // Other options as needed
});

var provider = services.BuildServiceProvider();
var crawler = provider.GetRequiredService&lt;Crawler&gt;();
</code></pre>
<p>Alternatively, you can instantiate <code>Crawler</code> and <code>Options</code> directly without DI:</p>
<pre><code class="lang-csharp">var options = new Options
{
    ItemFound = result =&gt; Console.WriteLine($&quot;Found: {result.Url}&quot;),
    // ...other options
};
var crawler = new Crawler(options);
</code></pre>
<h2 id="options-configuration">Options Configuration</h2>
<p>The <code>Options</code> class configures the crawler's behavior. Key properties include:</p>
<ul>
<li><code>ItemFound</code> (<code>Action&lt;ResultFile&gt;</code>): Callback invoked when a new page is discovered.</li>
<li><code>Allow</code> (<code>List&lt;string&gt;</code>): Regex patterns for URLs allowed to be crawled.</li>
<li><code>FollowOnly</code> (<code>List&lt;string&gt;</code>): Regex patterns for pages whose links should be followed.</li>
<li><code>Ignore</code> (<code>List&lt;string&gt;</code>): Regex patterns for URLs to ignore.</li>
<li><code>StartLocations</code> (<code>List&lt;string&gt;</code>): Initial URLs to start crawling from.</li>
<li><code>UrlReplacements</code> (<code>Dictionary&lt;string, string&gt;</code>): URL replacements during crawling.</li>
<li><code>NetworkCredentials</code> (<code>NetworkCredential</code>): Optional credentials for authentication.</li>
<li><code>UseDefaultCredentials</code> (<code>bool</code>): Use default system credentials.</li>
<li><code>Proxy</code> (<code>IWebProxy</code>): Optional proxy settings.</li>
</ul>
<p>Example callback method:</p>
<pre><code class="lang-csharp">void OnItemFound(ResultFile result)
{
    Console.WriteLine($&quot;Discovered: {result.Url} (Status: {result.StatusCode})&quot;);
    // Additional processing...
}
</code></pre>
<h2 id="basic-usage">Basic Usage</h2>
<p>Once configured, start the crawl process:</p>
<pre><code class="lang-csharp">crawler.StartCrawl();
</code></pre>
<p>The library will handle link discovery, content downloading, and result parsing. Your callback will be invoked for each discovered item.</p>
<h2 id="customization">Customization</h2>
<p>Spidey is built with extensibility in mind. The system is divided into the following subsystems, each replaceable via DI:</p>
<ol>
<li><strong>Content Parser (<code>IContentParser</code>)</strong> – Parses downloaded data into <code>ResultFile</code> objects.</li>
<li><strong>Engine (<code>IEngine</code>)</strong> – Handles HTTP requests and content downloading.</li>
<li><strong>Link Discoverer (<code>ILinkDiscoverer</code>)</strong> – Extracts links from content.</li>
<li><strong>Processor (<code>IProcessor</code>)</strong> – Processes parsed content (default: invokes your callback).</li>
<li><strong>Scheduler (<code>IScheduler</code>)</strong> – Manages work distribution.</li>
<li><strong>Pipeline (<code>IPipeline</code>)</strong> – Orchestrates the crawling process.</li>
</ol>
<p>To customize, implement the relevant interface from <code>Spidey.Engines.Interfaces</code> and register your implementation in the service provider. Note that if you call RegisterSpidey(), the registration is handled for you automatically. If you instantiate <code>Crawler</code> directly, you must compose the pipeline manually.</p>
<h2 id="faq">FAQ</h2>
<p><strong>Q: Can I run the crawler on multiple nodes?</strong></p>
<p>A: The default scheduler is single-node only. For distributed crawling, implement a custom scheduler (e.g., using a database or message queue) to coordinate work between instances.</p>
<h2 id="build-process">Build Process</h2>
<p>Requirements:</p>
<ul>
<li>Visual Studio 2022</li>
</ul>
<p>Clone the project and open the solution (<code>Spidey.sln</code>) in Visual Studio to build.</p>
<h2 id="license">License</h2>
<p>See <a href="../LICENSE">LICENSE</a> for details.</p>

</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/JaCraig/Spidey/blob/master/docfx_project/index.md/#L1" class="edit-link">Edit this page</a>
        </div>

        <div class="next-article d-print-none border-top" id="nextArticle"></div>

      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          <strong><a href='https://github.com/JaCraig/Spidey'>Open in Github</a></strong>
        </div>
      </div>
    </footer>
  </body>
</html>
